{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColBERTv2: Indexing & Search Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the relevant classes. As we'll see below, `Indexer` and `Searcher` are the key actors here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "from colbert import Indexer, Searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow here assumes an IR dataset: a set of queries and a corresponding collection of passages.\n",
    "\n",
    "The classes `Queries` and `Collection` provide a convenient interface for working with such datasets.\n",
    "\n",
    "We will use the *dev set* of the **LoTTE benchmark** we recently introduced in the ColBERTv2 paper. The dev and test sets contain several domain-specific corpora, and we'll use the smallest dev set corpus, namely `lifestyle:dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p downloads/\n",
    "\n",
    "# ColBERTv2 checkpoint trained on MS MARCO Passage Ranking (388MB compressed)\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P downloads/\n",
    "!tar -xvzf downloads/colbertv2.0.tar.gz -C downloads/\n",
    "\n",
    "# The LoTTE dev and test sets (3.4GB compressed)\n",
    "!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz -P downloads/\n",
    "!tar -xvzf downloads/lotte.tar.gz -C downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:01:03] #> Loading the queries from /home/ubuntu/ColBERT/downloads/lotte_passages/lifestyle/dev/questions.search.tsv ...\n",
      "[Jun 30, 23:01:03] #> Got 417 queries. All QIDs are unique.\n",
      "\n",
      "[Jun 30, 23:01:03] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 417 queries and 268,893 passages'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = '/home/ubuntu/ColBERT/downloads/lotte_passages'\n",
    "dataset = 'lifestyle'\n",
    "datasplit = 'dev'\n",
    "\n",
    "queries = os.path.join(dataroot, dataset, datasplit, 'questions.search.tsv')\n",
    "collection = os.path.join(dataroot, dataset, datasplit, 'collection.tsv')\n",
    "\n",
    "queries = Queries(path=queries)\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(queries)} queries and {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loaded 417 queries and 269k passages. Let's inspect one query and one passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are blossom end rot tomatoes edible?\n",
      "\n",
      "I don't know what J-Rock is definitively as it could mean a number of things, however if you mean a 'metal' style rock guitar sound that I believe to be popular over there, then it can be quite simple: Strip all of your pedals and effects, and turn all dials to zero; Turn the volume up as far as it will go; Keep the Bass and Treble dials high; Turn down any 'Middle' dials until the sound becomes scooped - bear in the mind that middle frequencies are important for clarity, so find a decent compromise; Bring up the gain until you get the sound you require; Finally, bring the volume down to a more manageable level. This will get you a lot of the way, but without any clearer description of what you want to sound like, then I can't offer any more help.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queries[24])\n",
    "print()\n",
    "print(collection[89852])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "For efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
    "\n",
    "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index.\n",
    "\n",
    "(With four Titan V GPUs, indexing should take about 13 minutes. The output is fairly long/ugly at the moment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300   # truncate passages at 300 tokens\n",
    "\n",
    "checkpoint = 'colbert-ir/colbertv2.0'\n",
    "index_name = f'{dataset}.{datasplit}.{nbits}bits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jun 30, 23:01:10] #> Note: Output directory /home/ubuntu/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits already exists\n",
      "\n",
      "\n",
      "[Jun 30, 23:01:10] #> Will delete 1 files already at /home/ubuntu/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits in 20 seconds...\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 300,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"colbert-ir\\/colbertv2.0\",\n",
      "    \"triples\": null,\n",
      "    \"collection\": {\n",
      "        \"provenance\": \"\\/home\\/ubuntu\\/ColBERT\\/downloads\\/lotte_passages\\/lifestyle\\/dev\\/collection.tsv\"\n",
      "    },\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"lifestyle.dev.2bits\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/home\\/ubuntu\\/ColBERT\\/docs\\/experiments\",\n",
      "    \"experiment\": \"notebook\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-06\\/30\\/23.01.00\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1\n",
      "}\n",
      "[Jun 30, 23:01:39] [0] \t\t # of sampled PIDs = 90887 \t sampled_pids[:3] = [218428, 5331, 156573]\n",
      "[Jun 30, 23:01:40] [0] \t\t #> Encoding 90887 passages..\n",
      "[Jun 30, 23:03:10] [0] \t\t avg_doclen_est = 151.3106231689453 \t len(local_sample) = 90,887\n",
      "[Jun 30, 23:03:12] [0] \t\t Creaing 65,536 partitions.\n",
      "[Jun 30, 23:03:12] [0] \t\t *Estimated* 40,686,367 embeddings.\n",
      "[Jun 30, 23:03:12] [0] \t\t #> Saving the indexing plan to /home/ubuntu/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/plan.json ..\n",
      "Clustering 13702168 points in 128D to 65536 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 1.03 s\n",
      "  Iteration 3 (56.24 s, search 55.06 s): objective=3.54652e+06 imbalance=1.297 nsplit=0       \n",
      "[Jun 30, 23:04:14] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 30, 23:04:56] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.036, 0.038, 0.035, 0.034, 0.035, 0.037, 0.036, 0.035, 0.034, 0.035, 0.035, 0.035, 0.037, 0.038, 0.036, 0.037, 0.032, 0.037, 0.035, 0.034, 0.035, 0.037, 0.034, 0.035, 0.034, 0.035, 0.036, 0.035, 0.039, 0.037, 0.036, 0.04, 0.038, 0.034, 0.035, 0.034, 0.037, 0.036, 0.036, 0.045, 0.036, 0.034, 0.036, 0.036, 0.037, 0.034, 0.033, 0.039, 0.038, 0.035, 0.034, 0.035, 0.04, 0.037, 0.035, 0.035, 0.038, 0.039, 0.045, 0.035, 0.035, 0.037, 0.036, 0.037, 0.039, 0.038, 0.04, 0.035, 0.034, 0.035, 0.038, 0.032, 0.034, 0.037, 0.035, 0.037, 0.037, 0.037, 0.037, 0.038, 0.039, 0.036, 0.036, 0.037, 0.033, 0.037, 0.036, 0.034, 0.033, 0.039, 0.036, 0.04, 0.035, 0.038, 0.037, 0.037, 0.04, 0.034, 0.036, 0.035, 0.034, 0.038, 0.036, 0.037, 0.039, 0.033, 0.037, 0.034, 0.036, 0.036, 0.036, 0.036, 0.037, 0.036, 0.037, 0.036, 0.038, 0.037, 0.034, 0.037, 0.035, 0.035, 0.038, 0.037, 0.034, 0.037, 0.037, 0.035]\n",
      "[Jun 30, 23:05:35] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[Jun 30, 23:05:35] #> Got bucket_cutoffs = tensor([-0.0292,  0.0002,  0.0296], device='cuda:0') and bucket_weights = tensor([-0.0512, -0.0135,  0.0139,  0.0518], device='cuda:0')\n",
      "[Jun 30, 23:05:35] avg_residual = 0.036224365234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:05:35] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:05:59] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 3,779,083 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:24, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:06:00] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:06:24] [0] \t\t #> Saving chunk 1: \t 25,000 passages and 4,073,198 embeddings. From #25,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:49, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:06:25] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:06:50] [0] \t\t #> Saving chunk 2: \t 25,000 passages and 4,442,623 embeddings. From #50,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:15, 25.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:06:51] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:07:15] [0] \t\t #> Saving chunk 3: \t 25,000 passages and 4,047,185 embeddings. From #75,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:40, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:07:16] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:07:40] [0] \t\t #> Saving chunk 4: \t 25,000 passages and 3,953,755 embeddings. From #100,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:05, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:07:41] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:08:04] [0] \t\t #> Saving chunk 5: \t 25,000 passages and 3,347,195 embeddings. From #125,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [02:30, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:08:05] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:08:29] [0] \t\t #> Saving chunk 6: \t 25,000 passages and 3,441,185 embeddings. From #150,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [02:54, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:08:29] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:08:53] [0] \t\t #> Saving chunk 7: \t 25,000 passages and 3,597,393 embeddings. From #175,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [03:18, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:08:54] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:09:17] [0] \t\t #> Saving chunk 8: \t 25,000 passages and 3,698,831 embeddings. From #200,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [03:42, 24.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:09:18] [0] \t\t #> Encoding 25000 passages..\n",
      "[Jun 30, 23:09:42] [0] \t\t #> Saving chunk 9: \t 25,000 passages and 3,739,499 embeddings. From #225,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [04:07, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:09:42] [0] \t\t #> Encoding 18893 passages..\n",
      "[Jun 30, 23:10:00] [0] \t\t #> Saving chunk 10: \t 18,893 passages and 2,636,639 embeddings. From #250,000 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [04:25, 24.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:10:01] [0] \t\t #> Checking all files were saved...\n",
      "[Jun 30, 23:10:01] [0] \t\t Found all files!\n",
      "[Jun 30, 23:10:01] [0] \t\t #> Building IVF...\n",
      "[Jun 30, 23:10:01] [0] \t\t #> Loading codes...\n",
      "[Jun 30, 23:10:01] [0] \t\t Sorting codes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 243.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:10:06] [0] \t\t Getting unique codes...\n",
      "[Jun 30, 23:10:06] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jun 30, 23:10:06] #> Building the emb2pid mapping..\n",
      "[Jun 30, 23:10:07] len(emb2pid) = 40756586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65536/65536 [00:03<00:00, 21399.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:10:10] #> Saved optimized IVF to /home/ubuntu/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/ivf.pid.pt\n",
      "[Jun 30, 23:10:10] [0] \t\t #> Saving the indexing metadata to /home/ubuntu/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits/metadata.json ..\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use.\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/ColBERT/docs/experiments/notebook/indexes/lifestyle.dev.2bits'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.get_index() # You can get the absolute path of the index, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
    "\n",
    "We can use the `queries` set we loaded earlier — or you can supply your own questions. Feel free to get creative! But keep in mind this set of ~300k lifestyle passages can only answer a small, focused set of questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:11:46] #> Loading collection...\n",
      "0M \n",
      "[Jun 30, 23:11:51] #> Loading codec...\n",
      "[Jun 30, 23:11:51] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 30, 23:11:52] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 30, 23:11:52] #> Loading IVF...\n",
      "[Jun 30, 23:11:52] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 888.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 30, 23:11:52] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 11/11 [00:01<00:00,  8.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# To create the searcher using its relative name (i.e., not a full path), set\n",
    "# experiment=value_used_for_indexing in the RunConfig.\n",
    "with Run().context(RunConfig(experiment='notebook')):\n",
    "    searcher = Searcher(index=index_name)\n",
    "\n",
    "\n",
    "# If you want to customize the search latency--quality tradeoff, you can also supply a\n",
    "# config=ColBERTConfig(ncells=.., centroid_score_threshold=.., ndocs=..) argument.\n",
    "# The default settings with k <= 10 (1, 0.5, 256) gives the fastest search,\n",
    "# but you can gain more extensive search by setting larger values of k or\n",
    "# manually specifying more conservative ColBERTConfig settings (e.g. (4, 0.4, 4096))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> How frequent are injuries in the Tour De France?\n",
      "0.040285587310791016\n",
      "\t [1] \t\t 17.8 \t\t On average, every Dutch person makes a trip by bicycle 5.6 times per week. This works out as an average across the whole population of 2.5 km cycled every day. That's the highest figure for any population in the world. If we assume that people cycle every day of their lives to the age of 80, and that they cycle that 2.5 km every day of their life, they will ride a bike for a total of 73000 km during their lifetime. Divide it into 6.5 million and you find a figure that a typical Dutch cyclist can expect a \"head/brain injury\" once every 90 lifetimes Note that it doesn't say how serious the injuries have to be in order to be included. However, it does give total numbers of head/brain injuries per year as 550 + 1600 = 2150 which is more than ten times the total deaths of cyclists per year from all types of injuries. For the sake of making the maths easy, let's lazily (and very inaccurately) assume that every death when cycling is due to a head injury. We then find that the risk of death due to head or brain injury when cycling is actually around once per 900 lifetimes. I have a teenager daughter too, and I understand being what it is like being a mom. I also understand what it is like to live in America, I always thought that anyone under 18 should wear a helmet. (Even though I never wore a helmet on a bike) I did make my daughter wear a helmet until she was 8, and then I started to teach how to safelt ride a bike, and then by the time she was about 9, she threw away her helmet and I felt confident about it. She bikes every day to school, which is 4 kilometers each way, and I feel 100 perecent comfident that she is safe and that feels good. then I really started to understand the safety, I bike about 6 kilometers every day, and not once have I fallen off a bike, the thing is that there are no potholes, and the Netherlands was designed for cyclists Another thing is before she takes that helmet off you need to teach her how to ride the bike safely, because without that teaching then she is 5 percent more likely to fall.\n",
      "\t [2] \t\t 17.6 \t\t European classics There are hundreds of outstanding rides in Europe for amateurs, but here's a few of the classics: Paris–Brest–Paris (PBP), the original audax: a 1,200 km ride from Paris to Brest on the Atlantic coast of Brittany, and back again, with a time limit of 90 hours. It has been run regularly since 1891 and in 2007 attracted more than 5,000 riders. For a long time this was the ride that European long-distance cyclists aspired to take part in. London–Edinburgh–London (LEL), a 1,400 km there-and-back ride between two of the UK's capital cities. Run 4-yearly since 1989. Vätternrundan, a 300 km tour of Lake Vättern in Sweden. Perhaps the most popular ride of this length in Europe, with over 15,000 riders. La Marmotte, a 174 km sportif from Bourg d'Oisans to Alpe d'Huez, passing over the famous cols of Glandon, Télégraphe, and Galibier en route, with over 8,000 m of climbing. Étape du Tour, a sportif in which amateurs ride one of the stages of the Tour de France. Some newer rides that may become classics in time: Hamburg–Berlin–Köln–Hamburg (HBKH), 1,500 km tour of northern Germany. 1001 Miglia Italia, a tour of Italy in 1,600 km (1,001 miles). Madrid–Gijon–Madrid, across Spain and back in 1,200 km.\n",
      "\t [3] \t\t 17.4 \t\t There is one more reason to add. They will be on the bike all day, every day, 4 days a week (or more). Around here, bike cops are used quite a bit \"down town\" and at almost all events in the area. These cops have to ride their bikes for hours at a time. A normal shift (around here) is 4 9 hour days, then 3 days off. By comparison the Tour de France's longest day is around 6 hours, and bike cops aren't professional athletes. Now they will spend a lot of time off the bike. But in addition to riding from point A to B, they will frequently have to jump curbs, ride down stairs, go on brick roads, cross tracks, ride though construction areas, and frequently take roads or paths that I would personally avoid. In these cases it's easy to see how their primary goal in choosing a bike may contain shocks where I would not.\n"
     ]
    }
   ],
   "source": [
    "query = queries[37]   # or supply your own query\n",
    "\n",
    "query = \"How frequent are injuries in the Tour De France?\"\n",
    "print(f\"#> {query}\")\n",
    "\n",
    "# Find the top-3 passages for this query\n",
    "import time\n",
    "now = time.time()\n",
    "results = searcher.search(query, k=3)\n",
    "after = time.time()\n",
    "print(after-now)\n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Search\n",
    "\n",
    "In many applications, you have a large batch of queries and you need to maximize the overall throughput. For that, you can use the `searcher.search_all(queries, k)` method, which returns a `Ranking` object that organizes the results across all queries.\n",
    "\n",
    "(Batching provides many opportunities for higher-throughput search, though we have not implemented most of those optimizations for compressed indexes yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [00:02<00:00, 152.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007222590686605988\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "rankings = searcher.search_all(queries, k=5).todict()\n",
    "after = time.time()\n",
    "t = after-now\n",
    "print(t/len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24367, 1, 16.015625),\n",
       " (35359, 2, 15.8125),\n",
       " (131623, 3, 15.75),\n",
       " (3789, 4, 15.7109375),\n",
       " (25089, 5, 15.703125)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings[30]  # For query 30, a list of (passage_id, rank, score) for the top-k passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
